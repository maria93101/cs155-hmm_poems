{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from HMM import unsupervised_HMM, HiddenMarkovModel\n",
    "import re\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poems(filename):\n",
    "    \n",
    "    lines = [] # 2d dictionary, each array is a split + cleaned line\n",
    "    words = {} # dictionary of a word, and its frequency\n",
    "    \n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if  len(line) < 10:\n",
    "            # Too short to be a valid line\n",
    "            continue\n",
    "        line = \"\".join(l for l in line if l not in string.punctuation)\n",
    "        line = line.lower()\n",
    "        line = line.split()\n",
    "        \n",
    "        lines.append(line)\n",
    "\n",
    "        for word in line:\n",
    "            try:\n",
    "                # add to frequency if the word is already in the dic\n",
    "                words[word] += 1\n",
    "            except KeyError:\n",
    "                # if not, add the word to the dic\n",
    "                words[word] = 1\n",
    "    return lines, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/shakespeare.txt\"\n",
    "lines, words = load_poems(file)\n",
    "obs, obs_map = parse_observations(\n",
    "lines)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in lines:\n",
    "    text += \" \".join(i)+\"\\n\"\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get syllable info from syllable_dictionary.txt\n",
    "def get_syllable():\n",
    "    # do stuff\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_learning(lines, n_states, n_iters):\n",
    "    '''\n",
    "    n_iters: Number of iterations we should go through.\n",
    "    n_states: Number of hidden states our HMM should have.\n",
    "    '''\n",
    "    # Train the HMM.\n",
    "    obs, obs_map =  parse_observations(lines)\n",
    "    flat_lines = [[item] for sublist in lines for item in sublist]\n",
    "    leHMM = unsupervised_HMM(obs, n_states, n_iters)\n",
    "    return leHMM, obs,obs_map \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_map_reverser(obs_map):\n",
    "    obs_map_r = {}\n",
    "\n",
    "    for key in obs_map:\n",
    "        obs_map_r[obs_map[key]] = key\n",
    "\n",
    "    return obs_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merp 0\n",
      "merp 1\n",
      "merp 2\n",
      "merp 3\n",
      "merp 4\n"
     ]
    }
   ],
   "source": [
    "testHMM, obs, obs_map  = unsupervised_learning(lines, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLines = 14\n",
    "obs_map_r = obs_map_reverser(obs_map)\n",
    "\n",
    "for i in range(14): # each poem is 14 lines long\n",
    "    emission = le40HMM.generate_emission(10) # each line is 10 words long\n",
    "    sentence = [obs_map_r[i] for i in emission[0]]\n",
    "\n",
    "    print(' '.join(sentence).capitalize() + '...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_HMM(hmmmmmm, filename):\n",
    "    \n",
    "    with open(filename+\".txt\", \"w+\") as filept:\n",
    "        filept.write(str(hmmmmmm.L)+\"\\n\")\n",
    "        filept.write(str(hmmmmmm.D)+\"\\n\")\n",
    "        for i in hmmmmmm.A:\n",
    "            line = \"\"\n",
    "            for j in i:\n",
    "                line += str(j) + \",\"\n",
    "            filept.write(line[:len(line)-1]+\"\\n\")\n",
    "        for i in hmmmmmm.O:\n",
    "            line = \"\"\n",
    "            for j in i:\n",
    "                line += str(j) + \",\"\n",
    "            filept.write(line[:len(line)-1]+\"\\n\")\n",
    "        \n",
    "\n",
    "def read_HMM(filename):\n",
    "    with open(filename+\".txt\", \"r\") as filept:\n",
    "        L = int(filept.readline())\n",
    "        D = int(filept.readline())\n",
    "        O = []\n",
    "        A = []\n",
    "        for i in range(L):\n",
    "            line = [float(x) for x in filept.readline().split(\",\")]\n",
    "            A.append(line)\n",
    "        for j in range(L):\n",
    "            line = [float(x) for x in filept.readline().split(\",\")]\n",
    "            O.append(line)\n",
    "    return HiddenMarkovModel(A, O)\n",
    "\n",
    "def parse_observations(lines):\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "\n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "\n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "\n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_HMM(le40HMM, \"40-iter-8-hidden-hmm\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le40HMM = read_HMM(\"40-iter-8-hidden-hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import RNN\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "import HMM_helper \n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "file = \"data/shakespeare.txt\"\n",
    "lines, words = load_poems(file)\n",
    "obs, obs_map = parse_observations(\n",
    "lines)\n",
    "\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(obs) - maxlen, step):\n",
    "    sentences.append(obs[i: i + maxlen])\n",
    "    next_chars.append(obs[i + maxlen])\n",
    "    \n",
    "sentences = sentences[0]\n",
    "x = np.zeros((len(sentences), maxlen, len(obs)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(obs)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char] = 1\n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "    \n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(obs))))\n",
    "\n",
    "#model.add(TimeDistributed(Dense(vocabulary)))\n",
    "#model.add(Activation('softmax'))\n",
    "model.add(Dense(len(obs), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])\n",
    "\"\"\"\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = obs[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        obs_map_r = obs_map_reverser(obs_map)\n",
    "    \n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "'''\n",
    "#Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path =  \"data/shakespeare.txt\"\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\\\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(hmmmmmm, obs_map):\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    # Finding the word most associated with each state. \n",
    "    counter = 0\n",
    "    for i in range(hmmmmmm.L):\n",
    "        index, value = max(enumerate(hmmmmmm.A[i]), key=operator.itemgetter(1))\n",
    "        print(i, obs_map_r[index])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(le40HMM, obs_map)\n",
    "# Parts of speech (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def script():\n",
    "    \n",
    "    file = open(\"data/shakespeare.txt\", 'r')\n",
    "    throwaway = [98, 125, 144]\n",
    "    sonnet_counter = 0\n",
    "    i = 0\n",
    "    all_pairs = []\n",
    "    temp = [[] for _ in range(7)]\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) < 10:\n",
    "            # Too short to be a valid line\n",
    "            if i != 0:\n",
    "                if sonnet_counter not in throwaway:\n",
    "                    all_pairs.extend(temp)\n",
    "                sonnet_counter += 1\n",
    "                i = 0\n",
    "                temp = [[] for _ in range(7)]\n",
    "            continue\n",
    "        line = \"\".join(l for l in line if l not in string.punctuation)\n",
    "        line = line.lower()\n",
    "        line = line.split()\n",
    "        \n",
    "        last = line[-1]\n",
    "        \n",
    "        if i == 0 or i == 2:\n",
    "            # a\n",
    "            temp[0].append(last)\n",
    "        elif i == 1 or i == 3:\n",
    "            #b\n",
    "            temp[1].append(last)\n",
    "        elif i==4 or i==6:\n",
    "            #c\n",
    "            temp[2].append(last)\n",
    "        elif i==5 or i==7:\n",
    "            #d\n",
    "            temp[3].append(last)\n",
    "            \n",
    "        elif i==8 or i==10:\n",
    "            #e\n",
    "            temp[4].append(last)\n",
    "            \n",
    "        elif i==9 or i==11:\n",
    "            #f\n",
    "            temp[5].append(last)\n",
    "            \n",
    "        elif i==12 or i==13:\n",
    "            #g\n",
    "            temp[6].append(last)\n",
    "            \n",
    "        i += 1\n",
    "        lines.append(line)\n",
    "\n",
    "                \n",
    "    all_pairs_dict = {}\n",
    "    for i, j in all_pairs:\n",
    "        if i not in all_pairs_dict:\n",
    "                    all_pairs_dict[i] = [j]\n",
    "                \n",
    "        if j not in all_pairs_dict:\n",
    "                    all_pairs_dict[j] = [i]\n",
    "                \n",
    "        # checking all against all other pairs\n",
    "        for k in all_pairs:\n",
    "            # If i or j is in k, this means we need to add things\n",
    "            if i in k or j in k:\n",
    "                for a in k:\n",
    "                    if a not in all_pairs_dict[i] and a != i:\n",
    "                        all_pairs_dict[i].append(a)\n",
    "                    if a not in all_pairs_dict[j] and a != j:\n",
    "                        all_pairs_dict[j].append(a)\n",
    "           \n",
    "    # Completing the graph. \n",
    "    for key, val in all_pairs_dict.items():\n",
    "        for i in val:\n",
    "            if key not in all_pairs_dict[i]:\n",
    "                all_pairs_dict[i].append(key)\n",
    "            for j in val:\n",
    "                if j not in all_pairs_dict[i]:\n",
    "                    all_pairs_dict[i].append(j)\n",
    "                    \n",
    "    return all_pairs, all_pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs, all_pairs_dict  = script()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
